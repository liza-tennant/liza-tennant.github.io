---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
--- 

You can also find my articles on my [Google Scholar profile](https://scholar.google.com/citations?hl=en&user=mtiYqfUAAAAJ&view_op=list_works&sortby=pubdate).

{% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %}

# Non-archival Conferences & Symposia:

- <ins>Elizaveta Tennant</ins>, Stephen Hailes, Mirco Musolesi. **Dynamics of Moral Behaviour in Heterogeneous Populations of Learning Agents** (Extended Abstract & Poster). [The 2025 Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM)](https://rldm.org/). Trinity College Dublin, Dublin, Ireland. June 2025.
- <ins>Elizaveta Tennant</ins>, Stephen Hailes, Mirco Musolesi. **Moral Alignment for AI Agents via Reinforcement Learning with Intrinsic Rewards** (Poster). [UK Multi-Agent Systems Symposium (UK-MAS)](https://www.turing.ac.uk/events/uk-multi-agent-systems-symposium-2025-uk-mas). The Alan Turing Institute, London, UK. March 2025.

# Conference Workshops: 

- <ins>Elizaveta Tennant</ins>, Stephen Hailes, Mirco Musolesi. **A Roadmap for Human-Agent Moral Alignment: Integrating Pre-defined Intrinsic Rewards and Learned Reward Models** (Short paper). [Workshop on Bi-Directional Human-AI Alignment](https://bialign-workshop.github.io/#/) at [ICLR'2025](https://iclr.cc/). Singapore. April 2025.
- <ins>Elizaveta Tennant</ins>, Stephen Hailes, Mirco Musolesi. **Moral Alignment for LLM Agents** (Long paper). [Workshop on Bi-Directional Human-AI Alignment](https://bialign-workshop.github.io/#/) at [ICLR'2025](https://iclr.cc/). Singapore. April 2025. 
