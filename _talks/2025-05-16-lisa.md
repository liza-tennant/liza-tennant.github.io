---
title: "Talk at LISA (London Initiative for Safe AI): Moral Alignment for RL and LLM Agents"
collection: talks
type: "Invited Talk"
permalink: /talks/2025-05-16-lisa
venue: "LISA, London"
date: 2025-05-16
location: "London, U.K."
---

Gave a 45-min talk dsicussing **Moral Alignment for RL and LLM Agents** (i.e., our 4 latest papers), as part of the [ARENA programme](https://www.arena.education/). In this talk I discussed agency in AI, existing approaches to Alignment (as described in [our preprint](https://arxiv.org/abs/2312.01818)), and my work on training and fine-tuning RL and LLM agents with intrinsic rewards, with particular focus on the unexpected findings from our papers at [IJCAI'23](https://doi.org/10.24963/ijcai.2023/36), [AIES'24](https://ojs.aaai.org/index.php/AIES/article/view/31736) and [ICLR'25](https://arxiv.org/abs/2410.01639). 

I was invited back to give another one of these talks for ARENA 6.0 in September 2025. 
